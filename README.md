# data_pipeline_Apr_2025

## (Planned) Project 1 - Real time data processing (Run in Docker + Orchestrate by Airflow)
1. Use API calls to pull from 2 different data sources
2. Real time processing using Kafka
3. Save processed data in PostgreSQL
4. Live report / analysis via ??? Spark ??? Redis ???
5. (Stregth) a trading algo to make it complete?


## (Planned) Project 2 - Data lake & Web scraping (Run in Docker + Orchestrate by Airflow)
1. Use `Faker` to generate fake data
2. Use `Selenium` in a script to automatically upload raw data onto personal cloud drive
3. Use `Selenium` in another script to detect landing of new raw data and download it via a data pipeline
4. Pass fake data into `Gemini API` to create summary report and save back into personal cloud drive
