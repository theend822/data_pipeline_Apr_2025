# data_pipeline_Mar_2025
Real time data processing - Finance Data


# (Planned) Project 1 - Real time data processing (Run in Docker + Orchestrate by Airflow)
1. Use API calls to pull from 2 different data sources
2. Real time processing using Kafka
3. Save processed data in PostgreSQL
4. Live report / analysis via ??? Spark ??? Redis ???
5. (Stregth) a trading algo to make it complete?


# (Planned) Project 2 - Data lake & Web scraping (Run in Docker + Orchestrate by Airflow)
1. Use `Faker` to generate fake data
2. Use `Selenium` to create a script to automatically upload raw data onto personal cloud drive
3. Use `Selenium` to create another script to detect landing of new raw data and process it via a data pipeline. Once done, upload back to personal cloud drive
